# 记忆系统语义检索升级方案

> 版本: 1.0
> 日期: 2026-02-09
> 状态: Phase 4a 已完成（2026-02-10）

---

## 1. 背景与目标

### 1.1 现状

当前记忆系统（`scripts/memory_runtime.mjs`）使用 **关键词子串匹配** 进行搜索：

```javascript
// 逐行扫描，大小写不敏感子串匹配 + 正则词边界
const queryLower = query.toLowerCase();
const exactWord = new RegExp(`\\b${escapeRegex(query)}\\b`, 'i');
```

**局限性：**
- "我喜欢 Python" 无法匹配 "我偏好 Python"
- 无法找到概念相关但措辞不同的记忆
- 所有匹配结果无相关性排序
- 上下文加载使用固定 3 天窗口，无法智能召回

### 1.2 目标

引入 **向量语义检索**，使媛媛能够：
- 理解语义相似性，而非仅匹配关键词
- 按相关性排序召回结果
- 在对话开始时自动注入相关记忆
- 在对话量增长后仍保持检索质量

### 1.3 策略选择：混合架构

采用 **混合方案**（Hybrid）而非全量迁移：

| 层 | 存储 | 用途 |
|---|---|---|
| 文件层（保留） | Markdown 文件 | 结构化数据：episodic 摘要、user-profile、mood-log、proactive-state |
| 向量层（新增） | LanceDB | 语义检索：对话片段、用户偏好、事实性记忆 |

**理由：**
- `proactive_daemon.mjs` 和 `self_reflection_runtime.mjs` 直接读取 Markdown 文件，全量迁移改动面过大
- 结构化数据（日期索引、情绪日志、JSON 状态）不适合向量检索
- 混合方案可渐进上线，降低风险

---

## 2. 技术方案

### 2.1 技术选型

| 组件 | 选择 | 理由 |
|---|---|---|
| 向量数据库 | LanceDB v0.23+ | OpenClaw 已有扩展；嵌入式、无需独立服务；文件级存储符合本地优先策略 |
| Embedding 模型 | OpenAI `text-embedding-3-small` | 1536 维；成本极低（$0.02/1M tokens）；质量足够 |
| 适配层 | 新建 `scripts/memory_semantic.mjs` | 封装 LanceDB 操作，提供与现有 CLI 兼容的接口 |

### 2.2 架构图

```
用户对话
  │
  ▼
┌─────────────────────────────────┐
│  memory_runtime.mjs (现有)       │
│  - write / load / compress       │
│  - search --mode keyword (默认)  │
│  - search --mode semantic (新增) │
│  - search --mode hybrid  (新增)  │
└──────┬──────────┬───────────────┘
       │          │
       ▼          ▼
  Markdown 文件   memory_semantic.mjs (新增)
  (episodic/      ┌──────────────────┐
   semantic/      │  embed()          │
   emotional/)    │  store()          │
                  │  search()         │
                  │  migrate()        │
                  └────────┬─────────┘
                           │
                           ▼
                     LanceDB (本地)
                     savc-core/memory/vector/
```

### 2.3 数据模型

```javascript
// LanceDB memories 表 schema
{
  id:         String,    // UUID v4
  text:       String,    // 记忆文本
  vector:     Float32[], // 1536 维 embedding
  importance: Number,    // 0-1，重要性评分
  category:   String,    // preference | fact | decision | entity | episodic
  source:     String,    // 来源文件路径或 "conversation"
  createdAt:  Number,    // Unix 时间戳
  updatedAt:  Number     // 最后更新时间戳
}
```

相比 OpenClaw 原始 schema，新增：
- `source` — 追踪记忆来源，便于调试和去重
- `updatedAt` — 支持时间衰减排序
- `category` 增加 `episodic` 类型 — 对应 episodic 摘要的向量化

### 2.4 搜索模式

#### keyword（现有，保持不变）
```bash
node scripts/memory_runtime.mjs search "TCP" --mode keyword
```

#### semantic（新增）
```bash
node scripts/memory_runtime.mjs search "网络连接原理" --mode semantic --limit 5
```
流程：query → embedding → LanceDB 向量搜索 → 按相似度排序返回

#### hybrid（新增，推荐默认）
```bash
node scripts/memory_runtime.mjs search "Python 偏好" --mode hybrid --limit 5
```
流程：同时执行 keyword + semantic，合并去重，按综合分排序

综合分计算：
```
score = α × semantic_similarity + (1 - α) × keyword_confidence
α = 0.7（可配置）
```

---

## 3. 实施步骤

### Step 1: 依赖安装与环境配置

**安装依赖：**
```bash
cd /home/min/SAVC/Self-aware-virtual-companion
pnpm add @lancedb/lancedb openai
```

**配置 Embedding API Key：**

在 `config/.env.local` 中添加：
```env
OPENAI_API_KEY=sk-...        # 用于 embedding，非 chat
EMBEDDING_MODEL=text-embedding-3-small
```

> 注意：这里的 OpenAI key 仅用于 embedding 生成，与主对话模型（Claude）无关。
> 如果已有 OpenAI key（用于其他用途），可复用同一个。

**向量存储目录：**
- 运行时自动创建 `savc-core/memory/vector/`，无需新增 `savc-core/memory/.gitignore`。

**验收标准：**
- [x] `pnpm ls @lancedb/lancedb` 显示已安装
- [x] `node -e "import('@lancedb/lancedb').then(m => console.log('ok'))"` 无报错
- [x] `.env.local` 支持 `OPENAI_API_KEY`/`EMBEDDING_MODEL` 配置（不入库）

---

### Step 2: 开发语义适配层 `memory_semantic.mjs`

新建 `scripts/memory_semantic.mjs`，封装以下接口：

```javascript
// 核心 API
export async function embed(text)                    // 文本 → 向量
export async function store(text, metadata)          // 存储记忆（含去重）
export async function search(query, { limit, minScore }) // 语义搜索
export async function remove(id)                     // 删除记忆
export async function stats()                        // 统计信息
export async function migrate(markdownDir)           // 批量导入 Markdown
```

**关键实现细节：**

1. **懒初始化** — 首次调用时才连接 LanceDB，避免启动开销
2. **去重检测** — 存储前先搜索，相似度 > 0.95 视为重复，跳过
3. **隐私过滤** — 复用现有 `memory_runtime.mjs` 的敏感信息过滤规则（密码、密钥、token）
4. **错误降级** — embedding API 不可用时，fallback 到关键词搜索并记录警告

**验收标准：**
- [x] `node scripts/memory_semantic.mjs store "用户喜欢用 Python 写脚本"` 成功写入
- [x] `node scripts/memory_semantic.mjs search "编程语言偏好"` 能召回上条记忆
- [x] `node scripts/memory_semantic.mjs search "Python"` 同样能召回（语义匹配）
- [x] 重复存储同一文本不会产生新记录

---

### Step 3: 集成到 `memory_runtime.mjs`

修改现有 `memory_runtime.mjs`，在不破坏现有接口的前提下增加语义能力：

**3a. search 命令扩展**

```bash
# 现有行为不变
node scripts/memory_runtime.mjs search "TCP"

# 新增 --mode 参数
node scripts/memory_runtime.mjs search "网络原理" --mode semantic
node scripts/memory_runtime.mjs search "Python" --mode hybrid
```

改动范围：`search` 函数（约 L588-651），新增分支调用 `memory_semantic.mjs`。

**3b. write 命令扩展（双写）**

每次 `write` 操作在写入 Markdown 的同时，将摘要文本同步写入 LanceDB：

```
write 流程:
  1. 写入 episodic/YYYY-MM-DD.md（现有）
  2. 更新 episodic/index.md（现有）
  3. [新增] 调用 semantic.store(summary, { category: 'episodic', source: filePath })
```

**3c. load 命令扩展（智能召回）**

在现有固定窗口加载之外，追加语义召回：

```
load 流程:
  1. 加载 user-profile + relationship（现有）
  2. 加载最近 N 天 episodic（现有）
  3. [新增] 如果提供了 --query 参数，追加语义召回 top-3 相关记忆
  4. Token 预算内拼接，超出时按优先级裁剪
```

**验收标准：**
- [x] `search` 无 `--mode` 参数时行为与之前完全一致
- [x] `write` 后 LanceDB 中能查到对应记忆
- [x] `load --query "之前聊的技术话题"` 能召回相关 episodic 记忆

---

### Step 4: 历史数据迁移

将现有 Markdown 记忆批量导入 LanceDB：

```bash
node scripts/memory_semantic.mjs migrate savc-core/memory/episodic
node scripts/memory_semantic.mjs migrate savc-core/memory/semantic
node scripts/memory_semantic.mjs migrate savc-core/memory/emotional
```

**迁移逻辑：**
1. 递归扫描目录下所有 `.md` 文件
2. 按段落（空行分隔）拆分为记忆片段
3. 过滤掉过短（< 20 字符）或纯格式行（标题、分隔线）
4. 为每个片段生成 embedding 并存入 LanceDB
5. 记录 `source` 字段指向原始文件路径
6. 输出迁移统计：总文件数、总片段数、去重跳过数

**验收标准：**
- [x] 迁移完成后 `memory_semantic.mjs stats` 显示合理的记忆条数
- [x] 对已有记忆内容的语义搜索能正确召回
- [x] 迁移脚本可重复执行（幂等），不会产生重复记录

---

### Step 5: 下游脚本适配

**5a. `proactive_daemon.mjs` — 话题跟进升级**

当前（L178-196）通过读取 `episodic/index.md` 做关键词匹配来选择跟进话题。
改为：调用语义搜索获取最近有意义的话题，提升跟进质量。

```javascript
// Before: 从 index.md 正则提取 topic
// After:  semantic.search("最近用户关心的话题", { limit: 3 })
```

**5b. `self_reflection_runtime.mjs` — 反思素材召回**

当前（L143-171）加载当天 episodic 文件做反思。
增加：语义搜索补充相关历史记忆，使反思更有纵深。

**5c. Memory Manager Skill 更新**

更新 `savc-core/skills/memory-manager/SKILL.md`：
- 新增 `semantic_search` 触发器描述
- 更新输出格式说明（增加 similarity score）

**验收标准：**
- [x] proactive daemon 的话题跟进使用语义搜索
- [x] self-reflection 能引用历史相关记忆
- [x] Phase 1/3 回归通过；Phase 2 live env 缺失项保持既有阻塞结论

---

### Step 6: 调优与监控

**6a. 相似度阈值调优**

| 参数 | 初始值 | 说明 |
|---|---|---|
| `minScore`（搜索） | 0.3 | 低于此分的结果不返回 |
| `duplicateThreshold` | 0.95 | 高于此分视为重复 |
| `hybridAlpha` | 0.7 | hybrid 模式中语义权重 |

上线后根据实际召回质量调整，建议记录每次搜索的 query + 返回结果 + 是否被采用，作为调优依据。

**6b. 成本监控**

在 `memory_semantic.mjs` 中记录 embedding API 调用次数和 token 消耗：

```
savc-core/memory/vector/usage.log
格式: [ISO时间] [操作] [tokens] [耗时ms]
```

预估月成本（基于 text-embedding-3-small）：
- 每天 50 次对话 × 平均 100 tokens/次 = 5,000 tokens/天
- 月消耗 ≈ 150,000 tokens ≈ **$0.003/月**

**6c. 健康检查**

新增 CLI 命令：
```bash
node scripts/memory_semantic.mjs health
# 输出: DB 连接状态、记忆总数、最近写入时间、embedding API 可用性
```

**验收标准：**
- [x] `usage.log` 正确记录每次 API 调用
- [x] `health` 命令输出所有检查项状态

---

### Step 7: 实施记录（2026-02-10）

已执行命令（节选）：

```bash
pnpm add @lancedb/lancedb openai
pnpm ls @lancedb/lancedb
bash scripts/test_phase4a.sh
```

结果：
- `scripts/test_phase4a.sh`：PASS（18/18）
- 包含 `memory_runtime` 三模式、双写、`load --query`、`migrate` 幂等、`health`、`usage.log` 与 Phase1/3 回归验证

---

## 4. 风险与应对

| 风险 | 影响 | 应对 |
|---|---|---|
| OpenAI Embedding API 不可用 | 语义搜索失败 | 自动降级到关键词搜索；`memory_semantic.mjs` 内置 fallback 逻辑 |
| LanceDB 文件损坏 | 向量数据丢失 | Markdown 文件层仍完整可用；向量数据可从 Markdown 重新迁移 |
| Embedding 成本超预期 | 费用增加 | 监控 `usage.log`；后续可切换到本地 embedding 模型（如 Ollama） |
| 迁移过程中断 | 部分数据未导入 | 迁移脚本幂等设计，可安全重跑 |
| 语义搜索召回不准 | 用户体验下降 | hybrid 模式保底；可调 `minScore` 阈值；保留关键词搜索作为 fallback |

---

## 5. 后续演进方向

完成本方案后，可进一步考虑：

1. **本地 Embedding 模型** — 通过 Ollama 运行本地 embedding（如 `nomic-embed-text`），彻底消除外部 API 依赖和成本
2. **记忆重要性衰减** — 引入时间衰减因子，近期记忆权重更高：`final_score = similarity × decay(age)`
3. **记忆聚类与整理** — 定期对向量空间做聚类，自动发现主题并生成摘要
4. **Auto-Capture 集成** — 参考 OpenClaw memory-lancedb 的 auto-capture 规则，在对话结束时自动提取值得记住的信息
5. **Auto-Recall 集成** — 在对话开始前自动注入相关记忆到系统提示词，无需手动 `load --query`

---

## 6. 文件变更清单

| 操作 | 文件 | 说明 |
|---|---|---|
| 新建 | `scripts/memory_semantic.mjs` | 语义适配层 |
| 修改 | `scripts/memory_runtime.mjs` | 增加 `--mode` 参数，双写逻辑 |
| 修改 | `scripts/proactive_daemon.mjs` | 话题跟进改用语义搜索 |
| 修改 | `scripts/self_reflection_runtime.mjs` | 反思素材增加语义召回 |
| 修改 | `savc-core/skills/memory-manager/SKILL.md` | 更新技能描述 |
| 修改 | `package.json` | 新增 `@lancedb/lancedb`、`openai` 依赖 |
| 修改 | `config/.env.example` | 新增 embedding 与语义检索相关环境变量 |
| 运行时生成 | `savc-core/memory/vector/` | LanceDB 数据目录与 usage.log |
| 新建 | `tests/skills/memory-semantic.test.mjs` | 语义搜索测试 |
| 新建 | `scripts/test_phase4a.sh` | Phase 4a 自动化验收脚本 |
